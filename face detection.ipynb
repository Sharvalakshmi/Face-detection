{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82eb3a0c-2fdc-4049-b5a1-6d41a08b5b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def load_data(data_path=\"dataset\"):\n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    labels, faces_data = [], []\n",
    "    \n",
    "    for person_name in os.listdir(data_path):\n",
    "        person_folder = os.path.join(data_path, person_name)\n",
    "        if not os.path.isdir(person_folder):\n",
    "            continue\n",
    "        for image_name in os.listdir(person_folder):\n",
    "            image_path = os.path.join(person_folder, image_name)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is None:\n",
    "                continue\n",
    "            faces = face_detector.detectMultiScale(image, 1.1, 5, minSize=(50, 50))\n",
    "            if len(faces) == 0:\n",
    "                continue\n",
    "            x, y, w, h = faces[0]\n",
    "            face_resized = cv2.resize(image[y:y+h, x:x+w], (100, 100))\n",
    "            faces_data.append(face_resized.flatten())\n",
    "            labels.append(person_name)\n",
    "    \n",
    "    return np.array(faces_data), np.array(labels)\n",
    "\n",
    "def train_model():\n",
    "    faces_data, labels = load_data()\n",
    "    if len(set(labels)) < 2:\n",
    "        messagebox.showerror(\"Error\", \"At least two different people are required for training.\")\n",
    "        return\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "    scaler = StandardScaler()\n",
    "    faces_data = scaler.fit_transform(faces_data)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(faces_data, labels, test_size=0.2, random_state=42)\n",
    "    model = SVC(kernel=\"linear\", probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    pickle.dump(model, open(\"face_recognition_model.pkl\", \"wb\"))\n",
    "    pickle.dump(le, open(\"label_encoder.pkl\", \"wb\"))\n",
    "    pickle.dump(scaler, open(\"scaler.pkl\", \"wb\"))\n",
    "    messagebox.showinfo(\"Success\", \"Model trained successfully!\")\n",
    "\n",
    "def recognize():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    model = pickle.load(open(\"face_recognition_model.pkl\", \"rb\"))\n",
    "    le = pickle.load(open(\"label_encoder.pkl\", \"rb\"))\n",
    "    scaler = pickle.load(open(\"scaler.pkl\", \"rb\"))\n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector.detectMultiScale(gray, 1.1, 5, minSize=(50, 50))\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            face_resized = cv2.resize(gray[y:y+h, x:x+w], (100, 100)).flatten().reshape(1, -1)\n",
    "            face_resized = scaler.transform(face_resized)\n",
    "            probabilities = model.predict_proba(face_resized)\n",
    "            best_class_index = np.argmax(probabilities)\n",
    "            confidence = probabilities[0][best_class_index]\n",
    "            predicted_name = le.inverse_transform([best_class_index])[0] if confidence > 0.6 else \"Unknown\"\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"{predicted_name} ({confidence*100:.1f}%)\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def add_new_face():\n",
    "    name = simpledialog.askstring(\"Input\", \"Enter new person's name:\")\n",
    "    if not name:\n",
    "        return\n",
    "    save_folder = os.path.join(\"dataset\", name)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    count = 0\n",
    "    \n",
    "    while count < 50:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector.detectMultiScale(gray, 1.1, 5, minSize=(50, 50))\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_resized = cv2.resize(gray[y:y+h, x:x+w], (100, 100))\n",
    "            cv2.imwrite(os.path.join(save_folder, f\"{count}.jpg\"), face_resized)\n",
    "            count += 1\n",
    "        cv2.imshow(\"Capturing Face\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    messagebox.showinfo(\"Success\", f\"Face dataset for {name} saved successfully!\")\n",
    "\n",
    "def main():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Face Recognition System\")\n",
    "    tk.Label(root, text=\"Face Recognition System\", font=(\"Arial\", 16)).pack(pady=10)\n",
    "    tk.Button(root, text=\"Train Model\", command=train_model, width=20).pack(pady=5)\n",
    "    tk.Button(root, text=\"Recognize Faces\", command=recognize, width=20).pack(pady=5)\n",
    "    tk.Button(root, text=\"Add New Person\", command=add_new_face, width=20).pack(pady=5)\n",
    "    tk.Button(root, text=\"Exit\", command=root.quit, width=20).pack(pady=5)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8607a5-3c9f-4d51-ad94-e96c55417af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.2329 - loss: 2.1248 - val_accuracy: 0.3304 - val_loss: 1.9367\n",
      "Epoch 2/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3s/step - accuracy: 0.2432 - loss: 1.8968 - val_accuracy: 0.2087 - val_loss: 1.7805\n",
      "Epoch 3/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 4s/step - accuracy: 0.2735 - loss: 1.8731 - val_accuracy: 0.3217 - val_loss: 1.6144\n",
      "Epoch 4/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 6s/step - accuracy: 0.4210 - loss: 1.5497 - val_accuracy: 0.3391 - val_loss: 1.4090\n",
      "Epoch 5/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 6s/step - accuracy: 0.4008 - loss: 1.3674 - val_accuracy: 0.4696 - val_loss: 1.2832\n",
      "Epoch 6/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.5042 - loss: 1.1980 - val_accuracy: 0.4957 - val_loss: 1.3732\n",
      "Epoch 7/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 4s/step - accuracy: 0.5992 - loss: 1.0811 - val_accuracy: 0.6435 - val_loss: 1.0184\n",
      "Epoch 8/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3s/step - accuracy: 0.6913 - loss: 0.7953 - val_accuracy: 0.5652 - val_loss: 0.9502\n",
      "Epoch 9/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3s/step - accuracy: 0.7737 - loss: 0.6159 - val_accuracy: 0.7652 - val_loss: 0.7818\n",
      "Epoch 10/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.8350 - loss: 0.4340 - val_accuracy: 0.7478 - val_loss: 0.8997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - accuracy: 0.1894 - loss: 2.2927 - val_accuracy: 0.2295 - val_loss: 1.9407\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - accuracy: 0.1945 - loss: 2.0510 - val_accuracy: 0.1967 - val_loss: 1.9949\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.2765 - loss: 1.8334 - val_accuracy: 0.2869 - val_loss: 1.7559\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.4116 - loss: 1.4976 - val_accuracy: 0.4672 - val_loss: 1.4557\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - accuracy: 0.4367 - loss: 1.3616 - val_accuracy: 0.5082 - val_loss: 1.2223\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.5562 - loss: 1.0811 - val_accuracy: 0.6066 - val_loss: 0.9715\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.7175 - loss: 0.6833 - val_accuracy: 0.6393 - val_loss: 1.2013\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 0.7653 - loss: 0.6743 - val_accuracy: 0.5410 - val_loss: 1.3052\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - accuracy: 0.6760 - loss: 0.8330 - val_accuracy: 0.7541 - val_loss: 0.8731\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.7740 - loss: 0.5269 - val_accuracy: 0.7459 - val_loss: 0.8643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tkinter import simpledialog, messagebox\n",
    "import tkinter as tk\n",
    "\n",
    "IMG_SIZE = 100  # Image size for CNN\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # Increase for better training\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_data(data_path=\"dataset\"):\n",
    "    labels, faces_data = [], []\n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    \n",
    "    for person_name in os.listdir(data_path):\n",
    "        person_folder = os.path.join(data_path, person_name)\n",
    "        if not os.path.isdir(person_folder):\n",
    "            continue\n",
    "        for image_name in os.listdir(person_folder):\n",
    "            image_path = os.path.join(person_folder, image_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            faces = face_detector.detectMultiScale(image, 1.1, 5, minSize=(50, 50))\n",
    "            if len(faces) == 0:\n",
    "                continue\n",
    "            x, y, w, h = faces[0]\n",
    "            face_resized = cv2.resize(image[y:y+h, x:x+w], (IMG_SIZE, IMG_SIZE))\n",
    "            faces_data.append(face_resized)\n",
    "            labels.append(person_name)\n",
    "\n",
    "    return np.array(faces_data), np.array(labels)\n",
    "\n",
    "# Build the CNN model using VGG16\n",
    "def build_model(num_classes):\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    for layer in base_model.layers[:-4]:\n",
    "        layer.trainable = False  # Freeze VGG16 layers\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.8),  # Dropout layer to prevent overfitting\n",
    "        Dense(num_classes, activation=\"softmax\")  # Output layer\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Train and save the CNN model\n",
    "def train_model():\n",
    "    faces_data, labels = load_data()\n",
    "    if len(set(labels)) < 2:\n",
    "        messagebox.showerror(\"Error\", \"At least two different people are required for training.\")\n",
    "        return\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "\n",
    "    # Normalize images\n",
    "    faces_data = faces_data / 255.0\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(faces_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = build_model(num_classes=len(set(labels)))\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model.save(\"face_recognition_cnn.h5\")\n",
    "    pickle.dump(le, open(\"label_encoder.pkl\", \"wb\"))\n",
    "\n",
    "    messagebox.showinfo(\"Success\", \"CNN Model trained successfully!\")\n",
    "\n",
    "# Real-time face recognition using CNN\n",
    "def recognize():\n",
    "    model = tf.keras.models.load_model(\"face_recognition_cnn.h5\")\n",
    "    le = pickle.load(open(\"label_encoder.pkl\", \"rb\"))\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        faces = face_detector.detectMultiScale(gray, 1.2, 6, minSize=(50, 50))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_resized = cv2.resize(frame[y:y+h, x:x+w], (IMG_SIZE, IMG_SIZE))\n",
    "            face_resized = np.expand_dims(face_resized / 255.0, axis=0)\n",
    "\n",
    "            predictions = model.predict(face_resized)\n",
    "            best_class_index = np.argmax(predictions)\n",
    "            confidence = predictions[0][best_class_index]\n",
    "            predicted_name = le.inverse_transform([best_class_index])[0] if confidence > 0.6 else \"Unknown\"\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"{predicted_name} ({confidence*100:.1f}%)\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Face Recognition\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Add a new face dataset\n",
    "\n",
    "def add_new_face():\n",
    "    name = simpledialog.askstring(\"Input\", \"Enter new person's name:\")\n",
    "    if not name:\n",
    "        return\n",
    "    save_folder = os.path.join(\"dataset\", name)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    count = 0\n",
    "    \n",
    "    while count < 100:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector.detectMultiScale(gray, 1.1, 5, minSize=(50, 50))\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_resized = cv2.resize(frame[y:y+h, x:x+w], (IMG_SIZE, IMG_SIZE))  # Keep it colored (RGB)\n",
    "            cv2.imwrite(os.path.join(save_folder, f\"{count}.jpg\"), face_resized)\n",
    "            count += 1\n",
    "        cv2.imshow(\"Capturing Face\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Notify user to retrain the model\n",
    "    retrain = messagebox.askyesno(\"Training Required\", \"New face added! Do you want to retrain the model now?\")\n",
    "    if retrain:\n",
    "        train_model()\n",
    "    else:\n",
    "        messagebox.showinfo(\"Reminder\", \"Retrain the model manually before recognition.\")\n",
    "\n",
    "  \n",
    "\n",
    "# GUI for the application\n",
    "def main():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Face Recognition System\")\n",
    "    tk.Label(root, text=\"Face Recognition System\", font=(\"Arial\", 16)).pack(pady=10)\n",
    "    tk.Button(root, text=\"Train Model\", command=train_model, width=20).pack(pady=5)\n",
    "    tk.Button(root, text=\"Recognize Faces\", command=recognize, width=20).pack(pady=5)\n",
    "    tk.Button(root, text=\"Add New Person\", command=add_new_face, width=20).pack(pady=5)\n",
    "    tk.Button(root, text=\"Exit\", command=root.quit, width=20).pack(pady=5)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96875022-a8af-4295-ac10-d1a6cf61bffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
